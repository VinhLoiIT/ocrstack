seed: 1
num_workers: 8
batch_size: 16
lr: 1.e-4
device: 'cpu'

# Number of iteration to train
iter_train: 2

# Number of iteration to eval
iter_eval: 1

# Number of iteration to warm up the model
iter_warmup: 1

# Visualize every `iter_visualize_every`
iter_visualize: 1

# Number of iteration to visualize during evaluation
num_iter_visualize: 1

checkpoint_dir: './logs'

# file name only. it will be placed into `checkpoint_dir`
log_file: 'train_log.txt'

# Write log every `log_interval` iteration(s)
log_interval: 10

monitor_metric: 'CER'  # Should be one of CER, WER, ACC
monitor_metric_type: 'lower'  # "lower" or "higher" value is better

# Transfer learning
pretrained_weight:  # pretrained weight path
pretrained_config:  # pretrained config path

# Resume training
resume_checkpoint:  # training checkpoint
continue_training: false  # resuming with new max_epochs (should be larger than the old one)

# Use Automatic Mixing Precision (AMP)
use_amp: false

# Gradient clipping value
clip_grad_value: 0.5
